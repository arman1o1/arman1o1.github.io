<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Building LLM Agents (the practical way) | arman1o1 blog</title>
    <meta name="description"
        content="A practical guide to building operational LLM agents: workflows, tools, orchestration, and guardrails." />
    <link rel="icon" type="image/png" href="../../favicon.png">
    <link rel="icon" type="image/svg+xml"
        href="data:image/svg+xml,<svg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 100 100'><text y='.9em' font-size='90'>ðŸ¤–</text></svg>">

    <!-- Google Fonts -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link
        href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&family=Playfair+Display:wght@600;700&display=swap"
        rel="stylesheet">

    <link rel="stylesheet" href="../../assets/styles.css">
    <script src="../../assets/theme.js"></script>

    <!-- Syntax Highlighting -->
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css" rel="stylesheet" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script
        src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>

    <!-- Mermaid.js -->
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true, theme: 'neutral' });
    </script>
</head>

<body>

    <header class="site-header">
        <div class="header-inner">
            <a href="../../index.html" class="brand">arman1o1 blog</a>

            <nav class="nav-links">
                <a href="../../index.html" class="nav-link">Home</a>
                <a href="../ai-ml/index.html" class="nav-link active">AI/ML</a>
                <a href="../movies/index.html" class="nav-link">Movies</a>
                <a href="../photography/index.html" class="nav-link">Photography</a>
                <a href="../life/index.html" class="nav-link">Life</a>
                <a href="../about/index.html" class="nav-link">About</a>
            </nav>

            <div class="header-toggle">
                <label class="switch" for="theme-toggle-checkbox" title="Toggle dark mode">
                    <input type="checkbox" id="theme-toggle-checkbox" aria-label="Toggle dark mode">
                    <span class="slider" aria-hidden="true"></span>
                </label>
            </div>
        </div>
    </header>

    <main id="main-content">
        <div class="container">
            <article class="post">
                <div class="tags">
                    <span class="tag">Agent</span>
                    <span class="tag">LLM</span>
                    <span class="tag">Tutorial</span>
                </div>
                <h1>Building LLM Agents (the practical way)</h1>
                <div class="post-meta">
                    <span class="post-date"><time datetime="2026-02-14">Feb 14, 2026</time></span>
                    <span>â€¢</span>
                    <span class="read-time">â˜• 15 min read</span>
                </div>

                <hr>

                <p>Agents aren't chatbots. They're systems that run workflows end-to-end: they decide what to do next,
                    call tools, recover from hiccups, and stop when the job is done.</p>
                <p>Based on the PDF <a
                        href="https://cdn.openai.com/business-guides-and-resources/a-practical-guide-to-building-agents.pdf"
                        target="_blank" rel="noopener noreferrer">A Practical Guide to Building Agents</a>.</p>

                <hr>

                <h2>TL;DR</h2>
                <ul>
                    <li>An agent is a workflow runner: an LLM in a loop, with tools and instructions, operating inside
                        guardrails.</li>
                    <li>Build agents when the work needs judgment, your rules are getting brittle, or the data is messy
                        (unstructured).</li>
                    <li>Start with a single agent and expand its toolset; go multi-agent only when complexity forces you
                        there.</li>
                    <li>"Orchestration" is just the control logic: runs, exits, handoffs, and how you manage state.</li>
                    <li>Tools are product surface area. Treat them like real APIs: stable contracts, tests, docs, and
                        explicit side effects.</li>
                    <li>Good instructions look like SOPs: numbered steps, explicit actions, and branches for edge cases.
                    </li>
                    <li>Guardrails are defense-in-depth: relevance + safety + PII + moderation + tool risk checks +
                        deterministic limits.</li>
                    <li>Human intervention isn't a failure mode; it's part of the design for early reliability and
                        high-risk actions.</li>
                </ul>

                <hr>

                <h2>Table of contents</h2>
                <ul>
                    <li><a href="#what-is-an-agent-and-what-isnt">What is an agent (and what isn't)</a></li>
                    <li><a href="#when-should-you-build-an-agent">When should you build an agent</a></li>
                    <li><a href="#agent-design-foundations-model--tools--instructions">Agent design foundations: model +
                            tools + instructions</a>
                        <ul>
                            <li><a href="#models-pick-capability-then-optimize">Models: pick capability, then
                                    optimize</a></li>
                            <li><a href="#tools-extend-the-agent-into-your-systems">Tools: extend the agent into your
                                    systems</a></li>
                            <li><a href="#instructions-reduce-ambiguity-on-purpose">Instructions: reduce ambiguity on
                                    purpose</a></li>
                        </ul>
                    </li>
                    <li><a href="#orchestration-how-the-whole-thing-runs">Orchestration: how the whole thing runs</a>
                        <ul>
                            <li><a href="#single-agent-systems-the-default">Single-agent systems (the default)</a></li>
                            <li><a href="#prompt-templates-complexity-without-prompt-sprawl">Prompt templates
                                    (complexity without prompt sprawl)</a></li>
                            <li><a href="#when-to-split-into-multiple-agents">When to split into multiple agents</a>
                            </li>
                            <li><a href="#multi-agent-patterns-manager-vs-decentralized">Multi-agent patterns: manager
                                    vs decentralized</a></li>
                        </ul>
                    </li>
                    <li><a href="#guardrails-layered-defense--human-intervention">Guardrails: layered defense + human
                            intervention</a>
                        <ul>
                            <li><a href="#types-of-guardrails-a-practical-menu">Types of guardrails (a practical
                                    menu)</a></li>
                            <li><a href="#how-to-build-guardrails-the-iteration-loop">How to build guardrails (the
                                    iteration loop)</a></li>
                            <li><a href="#optimistic-execution--tripwires-the-deep-cut">Optimistic execution + tripwires
                                    (the deep cut)</a></li>
                            <li><a href="#plan-for-human-intervention">Plan for human intervention</a></li>
                        </ul>
                    </li>
                    <li><a href="#putting-it-all-together-a-build-sequence-that-works">Putting it all together: a build
                            sequence that works</a></li>
                    <li><a href="#conclusion--next-steps">Conclusion + next steps</a></li>
                    <li><a href="#starter-checklist-screenshot-this">Starter checklist (screenshot this)</a></li>
                </ul>

                <hr>

                <h2 id="what-is-an-agent-and-what-isnt">What is an agent (and what isn't)</h2>
                <p>At a high level:</p>
                <ul>
                    <li>A <strong>workflow</strong> is a sequence of steps that must be executed to reach a goal
                        (resolve a support issue, book a reservation, ship a code change, generate a report).</li>
                    <li>An <strong>agent</strong> is a system that can run that workflow on the user's behalf with a
                        high degree of independence.</li>
                </ul>
                <p>The easiest way to make this concrete is to draw the boundary around "control of execution."</p>

                <h3>Agents do this</h3>
                <p>An agent:</p>
                <ol>
                    <li>Uses an LLM to manage execution and make decisions:
                        <ul>
                            <li>It decides what step comes next.</li>
                            <li>It recognizes when the workflow is complete.</li>
                            <li>It can correct itself when it notices a mistake.</li>
                            <li>If the workflow can't proceed safely, it can stop and hand control back to the user.
                            </li>
                        </ul>
                    </li>
                    <li>Has access to tools to interact with external systems:
                        <ul>
                            <li>Tools gather context (read data).</li>
                            <li>Tools take actions (write data, trigger side effects).</li>
                            <li>The agent chooses tools dynamically based on current state, while staying inside
                                guardrails.</li>
                        </ul>
                    </li>
                </ol>

                <h3>Not everything with an LLM is an agent</h3>
                <p>These are useful, but they're not agents in the workflow sense:</p>
                <ul>
                    <li>A <strong>simple chatbot</strong> that answers questions but doesn't control execution.</li>
                    <li>A <strong>single-turn LLM call</strong> (prompt in, text out) with no looping or tool use.</li>
                    <li>A <strong>classifier</strong> (sentiment, intent, relevance) that doesn't decide the next step
                        in a workflow.</li>
                </ul>

                <h3>The mental model you'll keep coming back to</h3>
                <p>An agent is basically a <strong>while-loop with judgment</strong>.</p>
                <p>That's the whole game: you build a loop that keeps going until an exit condition, and you give it the
                    right tools, instructions, and guardrails so that the loop is reliable.</p>

                <hr>

                <h2 id="when-should-you-build-an-agent">When should you build an agent</h2>
                <p>Agents shine when deterministic automation starts to feel like you're fighting reality.</p>
                <p>In plain terms, prioritize workflows that:</p>
                <ol>
                    <li>Need <strong>complex decision-making</strong> (nuanced judgment, exceptions, context-sensitive
                        calls).</li>
                    <li>Have <strong>rules that are painful to maintain</strong> (the rule engine is now a fragile
                        monster).</li>
                    <li>Depend heavily on <strong>unstructured data</strong> (natural language, documents, messy user
                        inputs).</li>
                </ol>

                <h3>A quick scoring table</h3>
                <p>Score each category from 0 to 2. If you're mostly 0s, you're probably better off with conventional
                    automation.</p>
                <table>
                    <thead>
                        <tr>
                            <th>Criterion</th>
                            <th>0 (low)</th>
                            <th>1 (medium)</th>
                            <th>2 (high)</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>Complex decision-making</td>
                            <td>Straight-line decisions</td>
                            <td>Some edge cases</td>
                            <td>Frequent nuance + exceptions</td>
                        </tr>
                        <tr>
                            <td>Difficult-to-maintain rules</td>
                            <td>Few stable rules</td>
                            <td>Rules change monthly</td>
                            <td>Rules change weekly + lots of exceptions</td>
                        </tr>
                        <tr>
                            <td>Unstructured data</td>
                            <td>Mostly structured fields</td>
                            <td>Mix of text + fields</td>
                            <td>Mostly text/docs/conversations</td>
                        </tr>
                    </tbody>
                </table>

                <h3>The "fraud investigator" analogy (why agents are different)</h3>
                <p>A traditional rules engine feels like a checklist: "If X and Y, then flag."</p>
                <p>An LLM-based agent is closer to a seasoned investigator: it can weigh context, notice subtle
                    patterns, and make a call even when no single rule was violated. That kind of reasoning is exactly
                    what helps in messy real-world workflows where the "right answer" depends on context.</p>

                <h3>When not to build an agent (yet)</h3>
                <p>If the workflow is stable, deterministic, and you can express it cleanly with normal code, do that
                    first. Agents add a new kind of complexity:</p>
                <ul>
                    <li>Non-determinism (the same input can produce different steps).</li>
                    <li>New failure modes (tool misuse, prompt injection, weird edge cases).</li>
                    <li>The need for guardrails and evaluation.</li>
                </ul>
                <p>Use agents where that tradeoff buys you something real.</p>

                <hr>

                <h2 id="agent-design-foundations-model--tools--instructions">Agent design foundations: model + tools +
                    instructions</h2>
                <p>At the most fundamental level, an agent has three building blocks:</p>
                <ul>
                    <li><strong>Model</strong>: the LLM that powers reasoning and decisions</li>
                    <li><strong>Tools</strong>: functions/APIs the agent can use to fetch context and take actions</li>
                    <li><strong>Instructions</strong>: explicit guidelines that define behavior and boundaries</li>
                </ul>
                <p>You can picture this as a triangle. If one corner is weak, the system feels unreliable:</p>
                <ul>
                    <li>Great model + weak tools = the agent can't actually do anything.</li>
                    <li>Great tools + weak instructions = the agent can do damage quickly.</li>
                    <li>Great instructions + weak model = the agent follows poorly, misunderstands, or fails to handle
                        nuance.</li>
                </ul>
                <p>Let's break down each corner.</p>

                <hr>

                <h3 id="models-pick-capability-then-optimize">Models: pick capability, then optimize</h3>
                <p>Different models have different strengths and tradeoffs: capability, latency, and cost. In agent
                    workflows, you might use different models for different steps (a cheap model for retrieval/triage, a
                    stronger model for hard decisions).</p>
                <p>But the trap is optimizing too early.</p>

                <h4>The baseline-first approach (it works for a reason)</h4>
                <ol>
                    <li>Prototype with the most capable model for every step.</li>
                    <li>Establish a performance baseline (how often does the agent do the right thing).</li>
                    <li>Swap in smaller/faster models step-by-step and see where quality holds.</li>
                </ol>
                <p>This keeps you from prematurely limiting the agent and gives you a clean way to diagnose failures:
                    "This step fails only when we downgrade the model" is actionable.</p>

                <h4>Model selection checklist</h4>
                <p>Use this as a quick gut-check:</p>
                <ul>
                    <li><input type="checkbox" disabled> I have an eval set (even a small one) that represents real user
                        cases.</li>
                    <li><input type="checkbox" disabled> I picked a model that meets the accuracy target on that eval
                        set.</li>
                    <li><input type="checkbox" disabled> I measured latency across the full workflow, not just one call.
                    </li>
                    <li><input type="checkbox" disabled> I only downgraded models where the eval results stayed
                        acceptable.</li>
                    <li><input type="checkbox" disabled> I know which steps are "cheap" (retrieval, intent) vs
                        "expensive" (high-stakes decisions).</li>
                </ul>
                <p>Deep cut: in agents, model choice isn't just "smarter is better." A too-smart model with unclear
                    tools and weak guardrails can be more dangerous than a smaller model in a constrained sandbox.
                    Capability magnifies whatever system you built.</p>

                <hr>

                <h3 id="tools-extend-the-agent-into-your-systems">Tools: extend the agent into your systems</h3>
                <p>Tools are how your agent touches reality.</p>
                <p>In modern systems, tools usually mean calling APIs. For legacy systems without good APIs, agents can
                    also interact through UIs (web/app) using computer-use style capabilities, basically "doing what a
                    human would do" in the interface.</p>
                <p>Either way: treat tools like you would any production API surface area.</p>

                <h4>Standardize tool definitions (future you will thank you)</h4>
                <p>As tool count grows, ad-hoc tools become a maintenance nightmare. Standardization buys you:</p>
                <ul>
                    <li>Discoverability (people know what exists)</li>
                    <li>Reuse (multiple agents can share the same tools)</li>
                    <li>Version management (you can evolve tools without breaking everything)</li>
                    <li>Fewer duplicate implementations</li>
                </ul>

                <h4>The three tool types you should name explicitly</h4>
                <p>Agents generally need three kinds of tools:</p>
                <ol>
                    <li><strong>Data tools</strong>: fetch context needed to do the work
                        <ul>
                            <li>examples: query a database, read a PDF, search the web, fetch CRM record</li>
                        </ul>
                    </li>
                    <li><strong>Action tools</strong>: make changes or trigger side effects
                        <ul>
                            <li>examples: send email/text, update a record, create a ticket, issue a refund</li>
                        </ul>
                    </li>
                    <li><strong>Orchestration tools</strong>: agents acting as tools for other agents
                        <ul>
                            <li>examples: "refund agent", "research agent", "writing agent"</li>
                        </ul>
                    </li>
                </ol>
                <p>Naming these categories matters because it forces you to talk about risk. Data tools are usually
                    lower risk; action tools can be high risk; orchestration tools amplify whatever you let them do.</p>

                <h4>Tool design rules (keep it boring)</h4>
                <ul>
                    <li><input type="checkbox" disabled> Give tools descriptive names that match user intent (not
                        internal jargon).</li>
                    <li><input type="checkbox" disabled> Define parameters clearly; avoid "blob of text" inputs when
                        structure works.</li>
                    <li><input type="checkbox" disabled> Document side effects ("this sends an email", "this charges a
                        card").</li>
                    <li><input type="checkbox" disabled> Make output types stable and machine-readable (structured
                        objects beat prose).</li>
                    <li><input type="checkbox" disabled> Add tests, especially for edge cases and failure states.</li>
                    <li><input type="checkbox" disabled> Handle reversibility: can this action be undone, and how.</li>
                    <li><input type="checkbox" disabled> Include idempotency keys for actions when possible (prevents
                        duplicate charges/tickets).</li>
                </ul>

                <h4>Tiny illustrative tool signatures (framework-agnostic)</h4>
                <pre><code class="language-text">get_customer_record(customer_id: str) -> CustomerRecord
initiate_refund(order_id: str, amount_usd: float, reason: str) -> RefundResult
create_support_ticket(subject: str, body: str, priority: str) -> Ticket</code></pre>
                <p>Notice what's missing: ambiguity. A good tool signature makes it hard for the agent to "almost" do
                    the right thing.</p>

                <hr>

                <h3 id="instructions-reduce-ambiguity-on-purpose">Instructions: reduce ambiguity on purpose</h3>
                <p>High-quality instructions matter for any LLM app, but agents are special: they run multi-step
                    workflows. Ambiguity compounds across steps.</p>
                <p>Good instructions are basically LLM-friendly SOPs.</p>

                <h4>Instruction best practices (what actually works)</h4>
                <ul>
                    <li>Use existing documents: SOPs, support scripts, policy docs, knowledge base articles.</li>
                    <li>Ask the agent to break down tasks into smaller steps.</li>
                    <li>Make each step correspond to a clear action or output.</li>
                    <li>Capture edge cases with conditional branches ("if missing X, ask for X").</li>
                </ul>

                <h4>A reusable instruction-writing template</h4>
                <p>Use this template when you turn a workflow into agent instructions:</p>
                <ol>
                    <li><strong>Goal</strong>: what "done" looks like.</li>
                    <li><strong>Inputs to collect</strong>: what info must be gathered from user or tools.</li>
                    <li><strong>Numbered steps</strong>: each step maps to one action (ask user, call tool, summarize,
                        etc).</li>
                    <li><strong>Branching rules</strong>: what to do when:
                        <ul>
                            <li>required info is missing</li>
                            <li>tool errors occur</li>
                            <li>the user asks a new question mid-flow</li>
                        </ul>
                    </li>
                    <li><strong>Stop conditions</strong>: when to end the run and respond to user.</li>
                    <li><strong>Safety rules</strong>: what the agent must never do; when to escalate.</li>
                </ol>

                <p>Here's a short example (illustrative, not copied from the PDF):</p>
                <pre><code class="language-text">Routine: "Refund request"

Goal: Either (a) successfully initiate a refund or (b) hand off to a human with a clear summary.

Steps:
1) Ask for order_id if missing.
2) Call get_order_details(order_id).
3) If order is not refundable, explain policy and offer alternatives.
4) If refund amount is > $200 (high risk), ask for confirmation and escalate to human.
5) Otherwise call initiate_refund(order_id, amount, reason).
6) Confirm to the user, including expected timeline.

If any tool fails:
- Retry once.
- If it fails again, stop and escalate with the error details and context.</code></pre>

                <h4>Using models to generate instructions (a very practical trick)</h4>
                <p>If you already have strong SOPs/policy docs, you can use advanced models to convert them into clean,
                    numbered agent instructions. The key is to demand "no ambiguity" and "explicit actions" in the
                    output. Think of it as drafting a first pass that humans then review and harden.</p>

                <hr>

                <h2 id="orchestration-how-the-whole-thing-runs">Orchestration: how the whole thing runs</h2>
                <p>Orchestration is the control plane: how your agent executes steps, chooses tools, handles errors, and
                    decides when to stop.</p>
                <p>The biggest mistake is jumping straight to a complex multi-agent architecture because it sounds cool.
                    In practice, teams usually succeed faster by starting incremental and keeping the orchestration
                    simple until they have proof they need more.</p>

                <hr>

                <h3 id="single-agent-systems-the-default">Single-agent systems (the default)</h3>
                <p>A single agent can handle a lot. You extend it by adding tools and refining instructions, without
                    introducing coordination overhead.</p>
                <p>Every orchestration approach needs a concept of a <strong>run</strong>: typically a loop that
                    executes until an exit condition is reached. Depending on your framework, the loop may pause at tool
                    calls (yielding control to an orchestrator) or execute tool calls inline. Common exit conditions
                    include:</p>
                <ul>
                    <li>The agent emits a tool call (if your runner pauses on tool calls)</li>
                    <li>The agent invokes a "final output" tool / returns a structured final result</li>
                    <li>The model returns a user-facing message with no tool calls</li>
                    <li>An error occurs</li>
                    <li>A maximum number of turns is reached</li>
                </ul>

                <h4>The run loop (Mermaid)</h4>
                <div class="mermaid">
                    flowchart TD
                    A["User input"] --> B["Agent run loop"]
                    B --> C{"Need tool?"}
                    C -- "yes" --> D["Call tool"]
                    D --> E["Update state/context"]
                    E --> B
                    C -- "no" --> F{"Exit condition met?"}
                    F -- "yes" --> G["Return final response"]
                    F -- "no" --> B
                </div>

                <h4>Framework-agnostic pseudocode</h4>
                <pre><code class="language-text">state = {messages: [...], context: {...}}

for turn in 1..MAX_TURNS:
  step = model.decide_next_step(state)

  if step.type == "tool_call":
    tool_result = tools.execute(step.tool_name, step.args)
    state = state.with(tool_result)
    continue

  if step.type == "final_message":
    return step.message

return "I couldn't finish safely. Here's what I tried... (handoff)"</code></pre>

                <p>If you only internalize one thing: the loop is the product. Everything else (tools, guardrails,
                    instructions) is there to make the loop act sane.</p>

                <hr>

                <h3 id="prompt-templates-complexity-without-prompt-sprawl">Prompt templates (complexity without prompt
                    sprawl)</h3>
                <p>As you add use cases, it's tempting to create a different prompt for each scenario. That gets messy
                    fast.</p>
                <p>A better strategy is to use a flexible base prompt that accepts policy variables. You maintain one
                    template and vary the inputs.</p>

                <h4>A small template example</h4>
                <pre><code class="language-text">You are a support agent for {{product_name}}.
Tone: {{tone_guidelines}}
Policy: Refunds allowed within {{refund_window_days}} days for {{eligible_reasons}}.

Follow this routine:
1) Identify intent.
2) Gather missing required fields: {{required_fields}}.
3) Use tools to verify eligibility.
4) Take the safest action that resolves the user request.
5) If action is high risk ({{high_risk_thresholds}}), escalate to a human.</code></pre>
                <p>This makes maintenance sane: policy changes become variable updates, not prompt rewrites.</p>

                <hr>

                <h3 id="when-to-split-into-multiple-agents">When to split into multiple agents</h3>
                <p>The general guidance is simple: maximize a single agent's capabilities first.</p>
                <p>Multiple agents can give nice separation of concerns, but they also introduce:</p>
                <ul>
                    <li>coordination overhead</li>
                    <li>more failure modes (handoff mistakes, context loss)</li>
                    <li>harder evaluation (which agent caused the issue)</li>
                </ul>
                <p>Split when you have clear signals that the single-agent approach is hitting a wall. Two practical
                    triggers:</p>
                <ol>
                    <li><strong>Complex logic</strong>: the prompt has so many conditionals that it becomes unscalable.
                    </li>
                    <li><strong>Tool overload</strong>: not just "too many tools", but too many similar/overlapping
                        tools that confuse selection.</li>
                </ol>

                <h4>"Go multi-agent?" checklist</h4>
                <ul>
                    <li><input type="checkbox" disabled> The base prompt is an if/then forest and hard to maintain.</li>
                    <li><input type="checkbox" disabled> The agent frequently selects the wrong tool among similar
                        tools.</li>
                    <li><input type="checkbox" disabled> Tool naming/params/descriptions are already clear but confusion
                        persists.</li>
                    <li><input type="checkbox" disabled> The workflow naturally splits into distinct domains with
                        minimal shared state.</li>
                    <li><input type="checkbox" disabled> You can evaluate each sub-agent independently with its own
                        success metrics.</li>
                </ul>
                <p>If most of these are false, stay single-agent longer.</p>

                <hr>

                <h3 id="multi-agent-patterns-manager-vs-decentralized">Multi-agent patterns: manager vs decentralized
                </h3>
                <p>Multi-agent systems can be modeled like graphs: nodes are agents, edges are either tool calls or
                    handoffs.</p>
                <p>Two broadly useful patterns:</p>

                <h4>1) Manager pattern (agents as tools)</h4>
                <p>One central "manager" agent keeps control, delegates to specialist agents via tool calls, then
                    synthesizes results. This is ideal when you want one consistent user experience and one place where
                    the workflow is controlled.</p>
                <div class="mermaid">
                    flowchart LR
                    U["User"] --> M["Manager agent"]
                    M --> S["Spanish specialist"]
                    M --> F["French specialist"]
                    M --> I["Italian specialist"]
                    S --> M
                    F --> M
                    I --> M
                    M --> U
                </div>
                <p>Even if your real use case isn't translation, the shape generalizes: the manager is the orchestrator;
                    specialists are narrow.</p>

                <h4>2) Decentralized pattern (handoffs)</h4>
                <p>In a decentralized pattern, agents are peers. One agent can hand off execution to another agent,
                    transferring the latest conversation state. This is great for triage: route the user to the right
                    specialist and let that specialist take over.</p>
                <div class="mermaid">
                    flowchart TD
                    U["User"] --> T["Triage agent"]
                    T --> O["Orders agent"]
                    T --> S["Sales agent"]
                    T --> R["Repairs agent"]
                    O --> U
                    S --> U
                    R --> U
                </div>

                <h4>Declarative vs code-first graphs (why this matters)</h4>
                <p>Some frameworks force you to define the whole workflow graph up front (every branch, loop, and
                    conditional). That can be visually nice, but it gets cumbersome as workflows become dynamic.</p>
                <p>A code-first approach lets you express workflow logic with normal programming constructs (if/else,
                    loops, functions) and build orchestration that adapts to state at runtime. For agent systems that
                    evolve quickly, that flexibility is a big deal.</p>

                <h4>Which pattern should you pick</h4>
                <ul>
                    <li>Pick <strong>manager</strong> when you want one agent to control execution and maintain a
                        unified "voice."</li>
                    <li>Pick <strong>decentralized</strong> when triage is the core problem and specialists should fully
                        take over.</li>
                </ul>
                <p>You can also hybridize: triage hands off to an agent that acts as a manager for sub-tasks inside its
                    domain.</p>

                <hr>

                <h2 id="guardrails-layered-defense--human-intervention">Guardrails: layered defense + human intervention
                </h2>
                <p>Guardrails are how you keep an agent safe, predictable, and on-brand in the real world.</p>
                <p>Think of guardrails as defense-in-depth. One guardrail is rarely enough. Multiple specialized
                    guardrails together make the system resilient.</p>
                <p>Also: guardrails aren't a substitute for basic software security. You still need strong
                    authentication/authorization, strict access controls, and normal security hygiene.</p>

                <hr>

                <h3 id="types-of-guardrails-a-practical-menu">Types of guardrails (a practical menu)</h3>
                <p>Here's a concrete menu of guardrails you can layer. The point isn't to implement all of them on day
                    one; it's to know what levers exist.</p>
                <table>
                    <thead>
                        <tr>
                            <th>Guardrail</th>
                            <th>What it prevents</th>
                            <th>Example trigger / behavior</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>Relevance classifier</td>
                            <td>Off-topic requests that derail the workflow</td>
                            <td>User asks unrelated trivia; agent redirects or refuses</td>
                        </tr>
                        <tr>
                            <td>Safety classifier</td>
                            <td>Jailbreaks / prompt injection attempts</td>
                            <td>"Ignore previous instructions..." => refuse and log</td>
                        </tr>
                        <tr>
                            <td>PII filter</td>
                            <td>Unnecessary exposure of personal data</td>
                            <td>Model output includes SSN/email; redact + warn</td>
                        </tr>
                        <tr>
                            <td>Moderation</td>
                            <td>Harmful/inappropriate content</td>
                            <td>Hate/harassment/violence => refuse and de-escalate</td>
                        </tr>
                        <tr>
                            <td>Tool safeguards (risk ratings)</td>
                            <td>High-stakes actions without oversight</td>
                            <td>Refund > threshold => require confirmation / human review</td>
                        </tr>
                        <tr>
                            <td>Rules-based protections</td>
                            <td>Known deterministic threats</td>
                            <td>Input length limits, blocklist regex, SQL injection patterns</td>
                        </tr>
                        <tr>
                            <td>Output validation</td>
                            <td>Brand/policy violations</td>
                            <td>Agent response violates policy; rewrite or block</td>
                        </tr>
                    </tbody>
                </table>
                <p>The key idea from the PDF: <strong>rate tools by risk</strong> (low/medium/high) based on read vs
                    write access, reversibility, required permissions, and financial impact. Then use those ratings to
                    decide when to pause for checks or escalate.</p>

                <h4>Guardrails in the workflow (Mermaid)</h4>
                <p>This shows the shape of layered checks around tool use:</p>
                <div class="mermaid">
                    flowchart TD
                    U["User input"] --> A["Agent"]
                    A --> G1["Rules checks (length, regex)"]
                    G1 --> G2["Relevance + safety classifiers"]
                    G2 --> G3["Moderation + PII checks"]
                    G3 --> D{"Safe + in-scope?"}
                    D -- "no" --> R["Refuse / ask to rephrase / handoff"]
                    D -- "yes" --> T{"Need tool?"}
                    T -- "no" --> O["Respond to user"]
                    T -- "yes" --> RISK{"Tool risk high?"}
                    RISK -- "yes" --> H["Human approval / extra checks"]
                    RISK -- "no" --> CALL["Call tool"]
                    H --> CALL
                    CALL --> A
                </div>

                <hr>

                <h3 id="how-to-build-guardrails-the-iteration-loop">How to build guardrails (the iteration loop)</h3>
                <p>The PDF's heuristic is practical and worth copying as a mindset (not as text):</p>
                <ol>
                    <li>Start with data privacy and content safety.</li>
                    <li>Add guardrails based on real-world edge cases and failures you observe.</li>
                    <li>Balance security and user experience as you iterate.</li>
                </ol>
                <p>In other words: you don't design perfect guardrails in a vacuum. You evolve them from incidents.</p>

                <h4>A production-friendly way to iterate</h4>
                <ul>
                    <li>Log guardrail triggers with enough context to debug (without logging sensitive data).</li>
                    <li>Track top failure modes weekly (wrong tool selection, off-topic, unsafe input, etc).</li>
                    <li>Convert each recurring failure into:
                        <ul>
                            <li>a new guardrail</li>
                            <li>a clearer instruction step</li>
                            <li>a tool redesign (better params, more structured outputs)</li>
                            <li>or a new human escalation rule</li>
                        </ul>
                    </li>
                </ul>

                <hr>

                <h3 id="optimistic-execution--tripwires-the-deep-cut">Optimistic execution + tripwires (the deep cut)
                </h3>
                <p>One interesting pattern in the PDF: treat guardrails as first-class and run them concurrently with
                    the agent's work.</p>
                <p>The idea is "optimistic execution":</p>
                <ul>
                    <li>Let the primary agent proceed to generate outputs or plan actions.</li>
                    <li>Run guardrails in parallel (relevance, safety, policy checks).</li>
                    <li>If a guardrail detects a violation, it triggers a "tripwire" that interrupts execution (think
                        exception).</li>
                </ul>
                <p>Why this is powerful:</p>
                <ul>
                    <li>It can keep the user experience fast (you don't block every step on a serial chain of checks).
                    </li>
                    <li>It centralizes enforcement: the agent can be helpful by default, but policies still win.</li>
                </ul>
                <p>Design detail that matters: your tripwire behavior should be user-friendly.</p>
                <ul>
                    <li>If you block, say what to do next ("Try rephrasing", "I can help with X").</li>
                    <li>If you escalate, say what will happen ("I'm handing this to a human because...").</li>
                </ul>

                <hr>

                <h3 id="plan-for-human-intervention">Plan for human intervention</h3>
                <p>Human intervention is a critical safeguard early in deployment. It helps you improve real-world
                    performance without wrecking user trust.</p>
                <p>Two triggers are especially important:</p>
                <ol>
                    <li><strong>Exceeding failure thresholds</strong>
                        <ul>
                            <li>The agent retries too many times.</li>
                            <li>It fails to understand intent after multiple attempts.</li>
                            <li>It hits max turns without progress.</li>
                        </ul>
                    </li>
                    <li><strong>High-risk actions</strong>
                        <ul>
                            <li>Sensitive, irreversible, or high-stakes operations:
                                <ul>
                                    <li>canceling orders</li>
                                    <li>authorizing large refunds</li>
                                    <li>making payments</li>
                                </ul>
                            </li>
                        </ul>
                    </li>
                </ol>

                <h4>A concrete escalation flow</h4>
                <p>When escalation triggers, make it deterministic:</p>
                <ol>
                    <li>Agent stops the run.</li>
                    <li>Agent produces a structured handoff packet:
                        <ul>
                            <li>user intent summary</li>
                            <li>known facts (tool outputs)</li>
                            <li>what it tried</li>
                            <li>what it needs next (missing fields / permissions)</li>
                            <li>why it escalated (threshold hit, high-risk tool, safety concern)</li>
                        </ul>
                    </li>
                    <li>Human reviews and takes action (or messages the user).</li>
                    <li>Agent resumes only if the human explicitly hands control back.</li>
                </ol>
                <p>This keeps the UX coherent and gives you clean training/eval data for the next iteration.</p>

                <hr>

                <h2 id="putting-it-all-together-a-build-sequence-that-works">Putting it all together: a build sequence
                    that works</h2>
                <p>This is the "start small, validate, grow" approach distilled into a concrete sequence.</p>

                <h3>Step 1: Pick one workflow that fits the criteria</h3>
                <p>Pick a workflow that scores high on at least one of:</p>
                <ul>
                    <li>complex decision-making</li>
                    <li>brittle rules</li>
                    <li>unstructured data</li>
                </ul>
                <p>Do not start with your most business-critical, highest-risk workflow unless you already have
                    guardrails and human oversight ready.</p>

                <h3>Step 2: Define success, failure modes, and an eval baseline</h3>
                <p>Write down:</p>
                <ul>
                    <li>what "done" means (for the user)</li>
                    <li>top failure modes you expect (wrong tool, policy violation, hallucinated claim, etc)</li>
                    <li>a small set of test cases (10-50 real-ish scenarios is enough to begin)</li>
                </ul>
                <p>Then run your first version against that set. You need a baseline before you can improve.</p>

                <h3>Step 3: List tools (data vs action) and assign risk ratings</h3>
                <p>Create an inventory:</p>
                <ul>
                    <li>Data tools: what context do we need.</li>
                    <li>Action tools: what changes do we need to make.</li>
                </ul>
                <p>For each action tool, assign:</p>
                <ul>
                    <li>risk: low/medium/high</li>
                    <li>reversibility: reversible/partially/irreversible</li>
                    <li>required permissions</li>
                    <li>financial impact</li>
                </ul>
                <p>Then decide your policy:</p>
                <ul>
                    <li>high-risk tools require confirmation + human review (at first)</li>
                    <li>medium-risk tools require confirmation or extra validation</li>
                    <li>low-risk tools can run automatically</li>
                </ul>

                <h3>Step 4: Write instructions as a numbered routine with branches</h3>
                <p>Use SOP-style instructions:</p>
                <ul>
                    <li>numbered steps</li>
                    <li>explicit actions per step</li>
                    <li>"if missing X, ask for X"</li>
                    <li>clear stop conditions</li>
                </ul>
                <p>Then do a pass where you remove any ambiguous language like "handle appropriately."</p>

                <h3>Step 5: Implement a single-agent loop with exit conditions</h3>
                <p>Keep orchestration simple:</p>
                <ul>
                    <li>max turns</li>
                    <li>tool call boundaries</li>
                    <li>structured output / final response</li>
                    <li>error handling and retry rules</li>
                </ul>
                <p>Measure end-to-end latency and success rate, not just individual calls.</p>

                <h3>Step 6: Add layered guardrails + human escalation</h3>
                <p>Start with:</p>
                <ul>
                    <li>deterministic limits (length, allowlists/blocklists where appropriate)</li>
                    <li>relevance/safety checks for user input</li>
                    <li>tool safeguards (risk ratings)</li>
                    <li>a clean human escalation path</li>
                </ul>
                <p>Add more based on what breaks in real usage.</p>

                <h3>Step 7: Iterate with real users, then consider multi-agent</h3>
                <p>Only after you have:</p>
                <ul>
                    <li>stable tool interfaces</li>
                    <li>clear instructions</li>
                    <li>guardrails that catch common failures</li>
                    <li>an eval loop that measures progress</li>
                </ul>
                <p>...then decide whether multi-agent is worth it.</p>
                <p>Multi-agent is an optimization for complexity. Don't pay that cost until you have to.</p>

                <hr>

                <h2 id="conclusion--next-steps">Conclusion + next steps</h2>
                <p>Agents represent a shift in workflow automation: systems that can reason through ambiguity, take
                    actions across tools, and run multi-step tasks with real autonomy.</p>
                <p>If you want reliable agents, keep the order straight:</p>
                <ol>
                    <li>Foundations first: capable models + well-defined tools + clear instructions.</li>
                    <li>Orchestration second: start simple, scale patterns as needed.</li>
                    <li>Guardrails always: layered defenses, tool risk controls, and human intervention where it
                        matters.</li>
                </ol>
                <p>Start small, validate with real users, and grow capabilities over time. That's how you get real value
                    without shipping chaos.</p>

                <hr>

                <h2 id="starter-checklist-screenshot-this">Starter checklist (screenshot this)</h2>
                <p>Use this as your "are we ready to ship a v1" list.</p>

                <h3>Use case</h3>
                <ul>
                    <li><input type="checkbox" disabled> Workflow needs judgment, messy data, or brittle rules</li>
                    <li><input type="checkbox" disabled> Clear definition of "done"</li>
                    <li><input type="checkbox" disabled> Known failure modes listed</li>
                </ul>

                <h3>Model</h3>
                <ul>
                    <li><input type="checkbox" disabled> Baseline established with a capable model</li>
                    <li><input type="checkbox" disabled> Evals exist (even small) and are representative</li>
                    <li><input type="checkbox" disabled> Downgrades only where quality holds</li>
                </ul>

                <h3>Tools</h3>
                <ul>
                    <li><input type="checkbox" disabled> Tool contracts are explicit and structured</li>
                    <li><input type="checkbox" disabled> Side effects documented</li>
                    <li><input type="checkbox" disabled> Tests cover success + failure cases</li>
                    <li><input type="checkbox" disabled> Action tools have risk ratings and reversibility noted</li>
                </ul>

                <h3>Instructions</h3>
                <ul>
                    <li><input type="checkbox" disabled> Numbered routine with explicit actions</li>
                    <li><input type="checkbox" disabled> Branches for missing info and common edge cases</li>
                    <li><input type="checkbox" disabled> Stop conditions clearly defined</li>
                </ul>

                <h3>Guardrails + operations</h3>
                <ul>
                    <li><input type="checkbox" disabled> Input safety/relevance checks in place</li>
                    <li><input type="checkbox" disabled> PII/moderation handled where needed</li>
                    <li><input type="checkbox" disabled> High-risk actions require confirmation and/or human approval
                    </li>
                    <li><input type="checkbox" disabled> Failure thresholds trigger escalation</li>
                    <li><input type="checkbox" disabled> Logging supports debugging without leaking sensitive data</li>
                </ul>

            </article>
        </div>
    </main>

    <footer class="site-footer">
        <div class="social-icons">
            <a href="https://github.com/arman1o1" target="_blank" rel="noopener noreferrer" aria-label="GitHub"
                class="icon github">
                <svg width="26" height="26" viewBox="0 0 16 16" fill="currentColor" xmlns="http://www.w3.org/2000/svg"
                    aria-hidden="true">
                    <path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 
            0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 
            1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 
            0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 
            0 0 .67-.21 2.2.82a7.65 7.65 0 0 1 2-.27c.68 0 
            1.36.09 2 .27 1.53-1.04 2.2-.82 
            2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 
            1.27.82 2.15 0 3.07-1.87 3.75-3.65 
            3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 
            2.2 0 .21.15.46.55.38A8.01 8.01 0 0 0 16 8c0-4.42-3.58-8-8-8z" />
                </svg>
            </a>
        </div>
    </footer>

</body>

</html>